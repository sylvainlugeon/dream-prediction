epoch training validation
0 [1.4759,3.7162] [0.8106,2.2078]
1 [1.3379,3.4877] [0.6794,2.1216]
2 [1.3499,3.4482] [0.6635,2.0650]
3 [1.1752,3.2927] [0.6433,2.0179]
4 [1.1827,3.3706] [0.6256,1.9861]
5 [1.0587,3.2879] [0.6059,1.9617]
6 [1.0315,3.2282] [0.5945,1.9424]
7 [0.9906,3.1094] [0.5890,1.9228]
8 [0.9346,2.9711] [0.5690,1.9037]
9 [0.8936,3.0121] [0.5572,1.8877]
10 [0.8533,3.0775] [0.5437,1.8689]
11 [0.8323,2.9463] [0.5222,1.8441]
12 [0.7698,2.9156] [0.5085,1.8421]
13 [0.7144,2.8880] [0.4920,1.8358]
14 [0.7149,2.9179] [0.4790,1.8355]
15 [0.6544,2.8670] [0.4719,1.8349]
16 [0.6389,2.8487] [0.4599,1.8379]
17 [0.6156,2.7911] [0.4635,1.8414]
18 [0.5956,2.8618] [0.4671,1.8396]
19 [0.5806,2.8001] [0.4688,1.8304]
20 [0.5449,2.7779] [0.4838,1.8335]
21 [0.5490,2.8699] [0.4689,1.8282]
22 [0.5163,2.7562] [0.5023,1.8358]
23 [0.5119,2.8560] [0.4862,1.8388]
24 [0.4964,2.8598] [0.4852,1.8310]
25 [0.4745,2.7624] [0.4201,1.8311]
26 [0.4584,2.8692] [0.4480,1.8307]
27 [0.4415,2.7922] [0.4859,1.8328]
28 [0.4390,2.8417] [0.4363,1.8377]
29 [0.4342,2.7895] [0.4499,1.8387]
30 [0.3970,2.8423] [0.4389,1.8408]
31 [0.4146,2.8061] [0.4318,1.8551]
32 [0.3737,2.8561] [0.4917,1.8656]
33 [0.3610,2.8895] [0.4476,1.8603]
34 [0.3503,2.8751] [0.4302,1.8551]
35 [0.3334,2.8645] [0.4823,1.8522]
36 [0.2972,2.8728] [0.4596,1.8556]
37 [0.2980,2.9316] [0.5475,1.8509]
38 [0.3013,2.8657] [0.5179,1.8528]
39 [0.2936,2.9389] [0.5220,1.8532]
40 [0.2969,2.8040] [0.5395,1.8520]
41 [0.2816,2.7931] [0.5322,1.8512]
42 [0.2873,2.9192] [0.5225,1.8512]
43 [0.2757,2.8141] [0.5513,1.8510]
44 [0.2713,2.9467] [0.5458,1.8527]
