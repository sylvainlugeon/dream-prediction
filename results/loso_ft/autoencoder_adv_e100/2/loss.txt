epoch training validation
0 [2.0187,3.5727] [1.4482,2.1872]
1 [1.5292,3.4486] [0.6783,2.1366]
2 [1.2643,3.3961] [0.6893,2.1131]
3 [1.1624,3.3225] [0.6273,2.0847]
4 [1.0693,3.3126] [0.6088,2.0509]
5 [1.0108,3.2059] [0.5943,2.0328]
6 [0.9222,3.1760] [0.5841,2.0075]
7 [0.8957,3.2090] [0.5539,1.9679]
8 [0.8212,3.0483] [0.5324,1.9459]
9 [0.7521,2.9855] [0.5129,1.9356]
10 [0.7104,2.9418] [0.5094,1.9169]
11 [0.6670,3.0467] [0.4895,1.9032]
12 [0.6587,2.9775] [0.4795,1.9012]
13 [0.5999,2.8539] [0.4729,1.8936]
14 [0.5878,2.9545] [0.4673,1.8864]
15 [0.5331,2.9381] [0.4554,1.8792]
16 [0.5130,2.9483] [0.4599,1.8676]
17 [0.4883,2.8346] [0.4467,1.8695]
18 [0.4683,2.7899] [0.4442,1.8650]
19 [0.4510,2.7534] [0.4385,1.8656]
20 [0.4459,2.8593] [0.4568,1.8624]
21 [0.4272,2.8378] [0.4947,1.8599]
22 [0.4020,2.8461] [0.4360,1.8551]
23 [0.3906,2.8438] [0.4466,1.8491]
24 [0.3593,2.8050] [0.4668,1.8458]
25 [0.3323,2.8137] [0.4472,1.8509]
26 [0.3346,2.8328] [0.4598,1.8485]
27 [0.3250,2.8446] [0.4833,1.8494]
28 [0.2908,2.9366] [0.4503,1.8565]
29 [0.2573,2.8020] [0.4719,1.8720]
30 [0.2668,2.8698] [0.4797,1.8782]
31 [0.2749,2.8194] [0.5542,1.8732]
32 [0.2413,2.9140] [0.4899,1.8709]
33 [0.2213,2.8810] [0.5984,1.8810]
34 [0.2116,2.8433] [0.4825,1.8812]
35 [0.2252,2.8635] [0.4778,1.8831]
36 [0.1918,2.8360] [0.4853,1.8841]
37 [0.2191,2.8236] [0.4867,1.8856]
38 [0.2065,2.8518] [0.4891,1.8862]
39 [0.1963,2.8617] [0.4944,1.8876]
40 [0.1855,2.8785] [0.4911,1.8882]
41 [0.2036,2.8890] [0.4909,1.8887]
