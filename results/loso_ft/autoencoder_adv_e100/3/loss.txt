epoch training validation
0 [1.6562,3.4844] [1.1556,2.0897]
1 [1.3505,3.5639] [0.6007,2.0374]
2 [1.2759,3.4442] [0.5880,2.0056]
3 [1.1299,3.2418] [0.5735,1.9685]
4 [1.0528,3.2931] [0.5774,1.9463]
5 [0.9825,3.2771] [0.5690,1.9330]
6 [0.9849,3.1375] [0.5723,1.9048]
7 [0.9304,3.0591] [0.5747,1.8860]
8 [0.8691,3.0460] [0.5601,1.8765]
9 [0.8126,2.9465] [0.5549,1.8640]
10 [0.7944,2.9719] [0.5400,1.8553]
11 [0.7631,2.8379] [0.5304,1.8498]
12 [0.7079,2.8175] [0.5324,1.8482]
13 [0.6907,2.8298] [0.5492,1.8461]
14 [0.6866,2.8225] [0.5046,1.8418]
15 [0.6636,2.8143] [0.4970,1.8432]
16 [0.6322,2.7591] [0.4942,1.8389]
17 [0.6082,2.7629] [0.4896,1.8429]
18 [0.6039,2.7343] [0.5042,1.8493]
19 [0.5680,2.6851] [0.4656,1.8530]
20 [0.5334,2.6962] [0.4716,1.8520]
21 [0.5248,2.6412] [0.5057,1.8523]
22 [0.5054,2.7032] [0.4720,1.8524]
23 [0.4789,2.6517] [0.4666,1.8488]
24 [0.5019,2.6887] [0.4670,1.8475]
25 [0.4715,2.6625] [0.4899,1.8462]
26 [0.4466,2.6293] [0.4903,1.8433]
27 [0.4413,2.5882] [0.4911,1.8437]
28 [0.4128,2.5923] [0.5061,1.8336]
29 [0.3972,2.6669] [0.5407,1.8219]
30 [0.3885,2.6262] [0.5056,1.8206]
31 [0.3791,2.6942] [0.4961,1.8211]
32 [0.3835,2.6187] [0.4871,1.8213]
33 [0.3697,2.6374] [0.4962,1.8212]
34 [0.3579,2.6521] [0.4995,1.8214]
35 [0.3655,2.6429] [0.4911,1.8221]
36 [0.3581,2.6275] [0.4905,1.8218]
37 [0.3633,2.6225] [0.5020,1.8216]
38 [0.3566,2.6116] [0.4811,1.8225]
