{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import sys \n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "\n",
    "sys.path.append('/home/lugeon/eeg_project/scripts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H009_E1_NREM_S03.img\n",
      "H009_E1_NREM_S05.img\n",
      "H009_E1_NREM_S06.img\n",
      "H009_E1_NREM_S07.img\n",
      "H009_E1_NREM_S09.img\n",
      "H009_E1_REM_S04.img\n",
      "H009_E1_REM_S08.img\n",
      "H009_E1_REM_S12.img\n",
      "H009_E2_NREM_S01.img\n",
      "H009_E2_NREM_S07.img\n",
      "H009_E2_NREM_S08.img\n",
      "H009_E2_NREM_S09.img\n",
      "H009_E1_NREM_S01.img\n",
      "H009_E1_NREM_S02.img\n"
     ]
    }
   ],
   "source": [
    "with np.load('/mlodata1/lugeon/dream_data/processed/healthy/images/H009.npz') as images:\n",
    "    for k in images:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237, 5, 32, 32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with np.load('/home/lugeon/eeg_project/H009.npz') as images:\n",
    "    h018 = images['H009_E1_REM_S12.img']\n",
    "\n",
    "h018.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "shw = ax.imshow(h018[3, 4, :, :], cmap='bone', vmin=-2, vmax=2);\n",
    "bar = plt.colorbar(shw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "shw = ax.imshow(h018[0, 1, :, :], cmap='bone', vmin=-2, vmax=2);\n",
    "bar = plt.colorbar(shw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('/mlodata1/yuecetue/eeg-dreams/YuceturkThesis2020/Data/hdf5/32_32_FFT_data_all_bands_120s_20sbj_log_z-score_stages23') as f:\n",
    "    for k in f:\n",
    "        print(f[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"frame_id\": shape (46176,), type \"<f4\">\n",
      "<HDF5 dataset \"images\": shape (46176, 5, 32, 32), type \"<f4\">\n",
      "<HDF5 dataset \"labels\": shape (46176,), type \"<f4\">\n",
      "<HDF5 dataset \"sleep_cycle\": shape (46176,), type \"<f4\">\n",
      "<HDF5 dataset \"sleep_stage\": shape (46176,), type \"<f4\">\n",
      "<HDF5 dataset \"subject_id\": shape (46176,), type \"<f4\">\n",
      "<HDF5 dataset \"subject_name\": shape (1,), type \"|S4\">\n",
      "<HDF5 dataset \"trial_id\": shape (46176,), type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('/mlodata1/lugeon/dream_data/processed/baseline/dataset_bis.h5') as f:\n",
    "    for k in f:\n",
    "        print(f[k])\n",
    "\n",
    "    frame_id = f['frame_id'][:]\n",
    "    subject_id = f['subject_id'][:]\n",
    "    trial_id = f['trial_id'][:]\n",
    "    images = f['images'][:]\n",
    "    labels = f['labels'][:]\n",
    "    sleep_cycle = f['sleep_cycle'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    14814\n",
       "3.0    10614\n",
       "4.0    10494\n",
       "1.0    10254\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sleep_cycle).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('/mlodata1/lugeon/dream_data/processed/healthy/dataset_small.h5') as f:\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "                'sid': f['subject_id'][:], \n",
    "                'tid': f['trial_id'][:], \n",
    "                'fid': f['frame_id'][:],\n",
    "                'ss': f['sleep_stage'][:]\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#refs#\n",
      "chanlocs\n",
      "condi\n",
      "datavr\n",
      "events\n",
      "numseg\n",
      "ordseg\n",
      "reactime\n",
      "srate\n",
      "stage\n",
      "subject\n"
     ]
    }
   ],
   "source": [
    "file_path = '/mlodata1/lugeon/dream_data/chuv/healthy/H009/H009_E1_NREM_S01.mat'\n",
    "with h5py.File(file_path,'r') as file:\n",
    "        for k in file:\n",
    "                print(k)\n",
    "        time_signal = np.array(file['subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[72],\n",
       "       [48],\n",
       "       [48],\n",
       "       [57]], dtype=uint16)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257, 60000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/mlodata1/lugeon/dream_data/chuv/healthy/M001/M001_E10_NREM_S4.mat'\n",
    "sio.loadmat(file_path)['datavr_2min'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 20])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(np.arange(10), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lugeon/miniconda3/envs/eeg-dream-conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import importlib\n",
    "sys.path.append('/home/lugeon/eeg_project/scripts')\n",
    "from training.representation import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'training.representation.models' from '/home/lugeon/eeg_project/scripts/training/representation/models/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = models.MaskedAutoEncoder(\n",
    "    in_channels = 1,\n",
    "    space_dim = 2,\n",
    "    time_dim = 2,\n",
    "    space_patch_size = 1, \n",
    "    time_patch_size = 1,\n",
    "    emb_size = 512, \n",
    "    masking_ratio = 0.5,\n",
    "    encoder_depth = 8, \n",
    "    decoder_depth = 2,\n",
    ")\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[1.0000e+00, 1.0000e+01],\n",
       "           [1.0000e+02, 1.0000e+03]]],\n",
       "\n",
       "\n",
       "         [[[1.0000e+04, 1.0000e+05],\n",
       "           [1.0000e+06, 1.0000e+07]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.0000e+00, 1.0000e+01],\n",
       "           [1.0000e+02, 1.0000e+03]]],\n",
       "\n",
       "\n",
       "         [[[1.0000e+04, 1.0000e+05],\n",
       "           [1.0000e+06, 1.0000e+07]]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = (torch.ones(8) * 10 ** torch.arange(8)).view(2, 1, 2, 2).unsqueeze(0).repeat(2, 1, 1, 1, 1)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/lugeon/eeg_project/draft/draft.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bk8s.iccluster.epfl.ch-4/home/lugeon/eeg_project/draft/draft.ipynb#ch0000050vscode-remote?line=0'>1</a>\u001b[0m idx \u001b[39m+\u001b[39;49m torch\u001b[39m.\u001b[39;49marange(batch_size) \u001b[39m*\u001b[39;49m n_patches\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "idx + torch.arange(n_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "n_patches = 8\n",
    "masking_ratio = 0.5\n",
    "patches = mae.patch_encoder(input)\n",
    "\n",
    "# shuffle patches independently for each element in the batch\n",
    "r = torch.rand(batch_size, n_patches)\n",
    "idx = torch.argsort(torch.rand(*r.shape), dim=-1)\n",
    "offset = (torch.arange(batch_size) * n_patches).unsqueeze(1).repeat(1, n_patches)\n",
    "idx += offset\n",
    "\n",
    "idx = torch.flatten(idx)\n",
    "\n",
    "shuffled = torch.flatten(patches, 0, 1)[idx, :] \\\n",
    "    .view(batch_size, n_patches, -1)\n",
    "                \n",
    "# keep only first patches \n",
    "n_masked = int(n_patches * masking_ratio)\n",
    "keep = n_patches - n_masked\n",
    "masked = shuffled[:, :keep, :]\n",
    "\n",
    "n_masked = int(n_patches * masking_ratio)\n",
    "mask_tokens = (torch.ones(1, 1, 512) * 1e10).repeat(batch_size, n_masked, 1)\n",
    "shuffled = torch.cat((masked, mask_tokens), dim=1) #Â (batch_size, n_patches, emb_size)\n",
    "shuffled = torch.flatten(shuffled, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 41494.38     -379.2755 -99437.055    9309.795 ]\n",
      "[ 4149394.5     -37973.652 -9943713.      930912.   ]\n",
      "[ 415.3789     -3.331728 -994.2929     93.77302 ]\n",
      "[ 414939.84     -3796.9463 -994371.2      93091.81  ]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "[ 41494.38     -379.2755 -99437.055    9309.795 ]\n",
      "[ 0.8544283   0.46188664 -0.91600645  0.77498114]\n",
      "[ 4.1933430e+01  8.5942864e-02 -9.9358765e+01  9.9910030e+00]\n",
      "[ 414939.84     -3796.9463 -994371.2      93091.81  ]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n"
     ]
    }
   ],
   "source": [
    "for b in shuffled:\n",
    "    print(b[:4].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unshuffle the patches w.r.t to shuffling indices\n",
    "unshuffled = torch.zeros_like(shuffled)\n",
    "unshuffled[idx] = shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "[ 415.3789     -3.331728 -994.2929     93.77302 ]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "[ 41494.38     -379.2755 -99437.055    9309.795 ]\n",
      "[ 414939.84     -3796.9463 -994371.2      93091.81  ]\n",
      "[ 4149394.5     -37973.652 -9943713.      930912.   ]\n",
      "[ 0.8544283   0.46188664 -0.91600645  0.77498114]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "[ 4.1933430e+01  8.5942864e-02 -9.9358765e+01  9.9910030e+00]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n",
      "[ 41494.38     -379.2755 -99437.055    9309.795 ]\n",
      "[ 414939.84     -3796.9463 -994371.2      93091.81  ]\n",
      "[1.e+10 1.e+10 1.e+10 1.e+10]\n"
     ]
    }
   ],
   "source": [
    "for b in unshuffled:\n",
    "    print(b[:4].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "unshuffled = unshuffled.view(batch_size, n_patches, -1)\n",
    "\n",
    "output = mae.patch_decoder(unshuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 3.8937e+09,  3.8937e+09],\n",
       "           [ 3.8937e+09, -1.5263e+02]]],\n",
       "\n",
       "\n",
       "         [[[ 3.8937e+09, -1.5234e+04],\n",
       "           [-1.5234e+05, -1.5234e+06]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[-4.4033e-01,  3.8937e+09],\n",
       "           [-1.5522e+01,  3.8937e+09]]],\n",
       "\n",
       "\n",
       "         [[[ 3.8937e+09, -1.5234e+04],\n",
       "           [-1.5234e+05,  3.8937e+09]]]]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[1.0000e+00, 1.0000e+01],\n",
       "           [1.0000e+02, 1.0000e+03]]],\n",
       "\n",
       "\n",
       "         [[[1.0000e+04, 1.0000e+05],\n",
       "           [1.0000e+06, 1.0000e+07]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.0000e+00, 1.0000e+01],\n",
       "           [1.0000e+02, 1.0000e+03]]],\n",
       "\n",
       "\n",
       "         [[[1.0000e+04, 1.0000e+05],\n",
       "           [1.0000e+06, 1.0000e+07]]]]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(unshuffled == patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = mae.patch_encoder(input)\n",
    "output = mae.patch_decoder(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-1.0688e-01,  1.6107e+00],\n",
       "           [ 1.8787e+01,  1.9055e+02]]],\n",
       "\n",
       "\n",
       "         [[[ 1.9082e+03,  1.9084e+04],\n",
       "           [ 1.9085e+05,  1.9085e+06]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[-1.0688e-01,  1.6107e+00],\n",
       "           [ 1.8787e+01,  1.9055e+02]]],\n",
       "\n",
       "\n",
       "         [[[ 1.9082e+03,  1.9084e+04],\n",
       "           [ 1.9085e+05,  1.9085e+06]]]]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[1.0000e+00, 1.0000e+01],\n",
       "           [1.0000e+02, 1.0000e+03]]],\n",
       "\n",
       "\n",
       "         [[[1.0000e+04, 1.0000e+05],\n",
       "           [1.0000e+06, 1.0000e+07]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.0000e+00, 1.0000e+01],\n",
       "           [1.0000e+02, 1.0000e+03]]],\n",
       "\n",
       "\n",
       "         [[[1.0000e+04, 1.0000e+05],\n",
       "           [1.0000e+06, 1.0000e+07]]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input = torch.ones(128, 10, 5, 32, 32).to(device)\n",
    "mae.to(device)\n",
    "\n",
    "output = mae(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches, idx = mae.encode(input, return_idx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40960])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10, 5, 32, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_patch_encoder = masked_autoencoder.VideoPatchEncoding(\n",
    "    in_channels = 5, \n",
    "    space_patch_size = 4, \n",
    "    time_patch_size = 2,\n",
    "    emb_size = 512)\n",
    "\n",
    "pos_adder = masked_autoencoder.PositionalAdder(emb_size=512, time_n_patches=5, space_n_patches=64)\n",
    "ts_pos = pos_adder.time_space_positions\n",
    "\n",
    "transformer_encoder = masked_autoencoder.AttentionNet(emb_size=512, depth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoPatchEncoding(\n",
       "  (projection): Conv3d(5, 512, kernel_size=(2, 4, 4), stride=(2, 4, 4))\n",
       ")"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_patch_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones(7, 10, 5, 32, 32)\n",
    "\n",
    "patches = video_patch_encoder(input)\n",
    "patches_pos = pos_adder(patches)\n",
    "\n",
    "output = transformer_encoder(patches_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 320, 512])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 320, 512])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_patch_encoder = masked_autoencoder.VideoPatchEncoding(\n",
    "    in_channels = 5, \n",
    "    space_patch_size = 4, \n",
    "    time_patch_size = 2,\n",
    "    emb_size = 512)\n",
    "\n",
    "pos_adder = masked_autoencoder.PositionalAdder(512, 5, 64)\n",
    "\n",
    "video_patch_decoder = masked_autoencoder.VideoPatchDecoding(\n",
    "    emb_size = 512,\n",
    "    in_channels = 5, \n",
    "    space_dim = 32,\n",
    "    time_dim = 10,\n",
    "    space_patch_size = 4, \n",
    "    time_patch_size= 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/lugeon/eeg_project/draft/draft.ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bk8s.iccluster.epfl.ch-4/home/lugeon/eeg_project/draft/draft.ipynb#ch0000048vscode-remote?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m video_patch_encoder(\u001b[39minput\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bk8s.iccluster.epfl.ch-4/home/lugeon/eeg_project/draft/draft.ipynb#ch0000048vscode-remote?line=1'>2</a>\u001b[0m x \u001b[39m=\u001b[39m pos_adder(x)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bk8s.iccluster.epfl.ch-4/home/lugeon/eeg_project/draft/draft.ipynb#ch0000048vscode-remote?line=2'>3</a>\u001b[0m x \u001b[39m=\u001b[39m video_patch_decoder(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/eeg-dream-conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/lugeon/miniconda3/envs/eeg-dream-conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/lugeon/miniconda3/envs/eeg-dream-conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/lugeon/miniconda3/envs/eeg-dream-conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/lugeon/miniconda3/envs/eeg-dream-conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/lugeon/miniconda3/envs/eeg-dream-conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/lugeon/miniconda3/envs/eeg-dream-conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/lugeon/miniconda3/envs/eeg-dream-conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/eeg_project/scripts/training/representation/masked_autoencoder.py:69\u001b[0m, in \u001b[0;36mVideoPatchDecoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///home/lugeon/eeg_project/scripts/training/representation/masked_autoencoder.py?line=65'>66</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m# (batch_size x n_patches x n_pixels)\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/lugeon/eeg_project/scripts/training/representation/masked_autoencoder.py?line=67'>68</a>\u001b[0m batch_size \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='file:///home/lugeon/eeg_project/scripts/training/representation/masked_autoencoder.py?line=68'>69</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mview(batch_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_channels, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtime_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspace_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspace_dim) \n\u001b[1;32m     <a href='file:///home/lugeon/eeg_project/scripts/training/representation/masked_autoencoder.py?line=69'>70</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m) \u001b[39m# (batch_size, time, n_channels, x, y)\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/lugeon/eeg_project/scripts/training/representation/masked_autoencoder.py?line=70'>71</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "x = video_patch_encoder(input)\n",
    "x = pos_adder(x)\n",
    "x = video_patch_decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57ad056f9be674611c76ca607e64a7703485de37232689043f87b90b104ba991"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
